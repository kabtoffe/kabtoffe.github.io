<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Automating Quote Post Creation with Feedly and Hexo</title>
    <url>/2026/02/07/Automating-Quote-Posts-with-Feedly-and-Hexo/</url>
    <content><![CDATA[I spend a lot of my time reading blog articles using my feed reader and wanted a lightweight way to capture interesting quotes and commentary directly into my blog. Rather than manually creating markdown files, I built, with a lot of assistance from Claude, a two-part automation system: a Node.js script that generates posts from parameters, and a browser bookmarklet that extracts article data from Feedly.
The basic idea is that when I read through my feed and see something interesting I can use Feedly’s native highlighting functionality and notes to add my commentary and then use that as a source for post generation.
How Hexo WorksI currently use Hexo for my blog. Hexo is a static site generator that converts markdown files into HTML. Here’s the basic workflow:

Source files: You write markdown files in source/_posts/ with YAML frontmatter metadata (title, date, tags, etc.)
Generation: Running hexo generate processes these files through templates and outputs static HTML to the public/ directory
Deployment: hexo deploy pushes the generated HTML to your hosting (in my case, GitHub Pages)

The key advantage is that your entire blog is version-controlled markdown files—no database, no complex tooling. Each post is just a file with metadata and content. I also do not need to worry about anyone hacking my site as there is no administrator login.
Creating the Quote Post ScriptTo automate post creation based on the article. I built a Node.js CLI script (tools/create-quote-post.js) that:

Accepts command-line parameters: author, URL, title, publication date, quote, and commentary
Generates proper frontmatter: Creates YAML metadata with tags, source information, and post date (today, not the article date)
Formats the markdown body: Renders the quote as a blockquote and includes your commentary
Writes the file: Creates a markdown file in source/_posts/ with a sanitized filename

Examplenode tools/create-quote-post.js \  --author &quot;Ben Thompson&quot; \  --url &quot;https://stratechery.com/...&quot; \  --title &quot;Microsoft and Software Survival&quot; \  --date &quot;2026-02-07&quot; \  --quote &quot;That, then, raises the most obvious bear case...&quot; \  --commentary &quot;Really interesting perspective&quot;

Generates a post with:

Frontmatter including source_author, source_url, and source_date (preserving the original article’s date)
Post date set to today
Proper YAML list formatting for tags
Quote rendered as markdown blockquote

Directory Structure: The scripts live in tools/ rather than Hexo’s scripts/ directory, which prevents Hexo from trying to load them as plugins during the build process.
This script serves as the base for the automation hook.
Extracting Data with a Feedly BookmarkletFor extracting the data off of Feedly there are a few options. Feedly does have an API but that is only available for the Enterprise plan. One option to create a workflow would have been to use IFTTT to create an automation using the Feedly applets and then call a web api with that. I might still do that as that might more mobile friendly, but for a first try I decided to just use the browser. As a simpler alternative to a browser addon, I made a bookmarklet that:

Extracts article data from Feedly’s DOM:
Title, URL, author&#x2F;publication name
Publication date (parsed from Feedly’s date format)
All highlighted text (grouped by paragraph to preserve structure)
Any notes&#x2F;comments I added in Feedly
Feedly board names as tags





Presents an editable form in a modal dialog where I can:
Review and edit all extracted data
Add or modify my comments further
Adjust tags before generating





Generates the bash command for the post creation, ready to copy and paste into my terminal

Part 2: Web Server Automation (Coming Soon)While the bookmarklet works well for one-off posts, the next step is building a lightweight web server that can:

Accept article data via HTTP (from a bookmarklet or API endpoint)
Generate and commit the post automatically
Trigger a blog rebuild and deployment
Return the published post URL

This would eliminate the need to manually run the script, enabling a one-click publishing workflow directly from Feedly.

Want to try it? The bookmarklet and script are open for customization. You can find:

The minified bookmarklet in BOOKMARKLET.md
The extraction logic in feedly-bookmarklet.js
The post creation script in create-quote-post.js

Note: At the moment the form is not quite usable inside Feedly, but the buttons work.
]]></content>
      <tags>
        <tag>Hexo</tag>
        <tag>NodeJS</tag>
        <tag>Automation</tag>
        <tag>Feedly</tag>
        <tag>Bookmarklet</tag>
      </tags>
  </entry>
  <entry>
    <title>ConvertTo-CustomObject function as a wrapper for Select-Object</title>
    <url>/2017/07/12/ConvertTo-CustomObject-function-as-a-wrapper-for-Select-Object/</url>
    <content><![CDATA[In yesterday’s post I introduced a custom advanced function for changing property names on objects. I also showed you how to use Select-Object to do the same. Because the Select-Object cmdlet is probably more robust than my little function I decided to try and rewrite the function to use Select-Object under the hood.
Select-Object accepts an array of hashtable objects with two key&#x2F;value pairs:

N is for the name of the property
E is the expression as a script block that need to be run to get the value

In order to use this functionality we need to figure out a way to generate these objects dynamically using our mapping table. The approach I took was to generate the code as strings and then use Invoke-Expression. Since we only need to do this once and not for every object I added a BEGIN block to my function:
BEGIN &#123;    $Expression = &quot;&quot;    $MappingTable.Keys | ForEach-Object &#123;        $Expression += (            @&quot;                @&#123;                    N=&quot;$($MappingTable[$_])&quot;                    E=&#123;`$_.$_&#125;                &#125;,        &quot;@        )    &#125;    $Expression = $Expression.Substring(0,$Expression.Length-1)    $SelectObjectParameter = Invoke-Expression $Expression&#125;
So for each key in the mapping table we generate a name and an expression to run. Eg. using a mapping table like @{ &quot;name&quot; = &quot;AccountName&quot;; &quot;telephone1&quot; = &quot;Phone&quot;  }generates the following string:
@&#123;    N=&quot;Phone&quot;    E=&#123;$_.telephone1&#125;&#125;,                    @&#123;    N=&quot;AccountName&quot;    E=&#123;$_.name&#125;&#125;
That string is exactly what we would feed Select-Object. To get it into actual PowerShell objects we need to feed it to Invoke-Expression. That gives us an array with two separate hashtables each with a N key with a string value and an E key with a script block value.
Once stored in a variable we can use this for each of the objects fed through the pipeline by using it in the PROCESS block of our advanced function:
PROCESS &#123;        $SourceObject | Select-Object -Property $SelectObjectParameter    &#125;  
And that is all there is to it. The function works exactly the same as the ‘quick and dirty’ one in the last post. You can find the full function here. Import it to your session and try this example: Get-Process | ConvertTo-CustomObject -MappingTable @{ &quot;ProcessName&quot; = &quot;Name&quot; }
]]></content>
      <tags>
        <tag>PowerShell</tag>
      </tags>
  </entry>
  <entry>
    <title>Blog moved to Github Pages</title>
    <url>/2017/06/11/Blog-moved-to-Github-Pages/</url>
    <content><![CDATA[Just a quick post to see that the blog publishing still works. I moved the blog from Azure to Github pages. All in all really simple. You just create a repository and publish via git. In my case as I was already using git for publishing all I needed to do was change the git endpoint, add a custom domain and point my DNS to github.
]]></content>
  </entry>
  <entry>
    <title>Dynamics 365 Document Template from Existing Word Document</title>
    <url>/2017/04/23/Dynamics-365-Document-Template-from-existing-Word-Document/</url>
    <content><![CDATA[For the impatient this is how you do it:

Extract the Dynamics 365 generated template from zip and add the CustomXML-part to your existing template.

I couldn’t find a post on how to create a new Dynamics 365 Document Template using an existing document or template. I’m sure many organisations have an existing template with all the right formatting, headers and footers they would like to use.
Knowing how .docx-files are structured and how Dynamics 365 stores the metadata it was easy enough to figure out. I thought I would share how with you.
Step by step
Create a new blank template from Dynamics 365 the way you would normally do it and save the file to your computer
Change the extension of the file to .zip and extract it
Open the existing document you would like to use with Dynamics 365
In your Word template click Developer &gt; XML Mapping Pane
In the drop down menu select (Add new part…)
Navigate to your extracted template and select the xml-file (item.xml) under the CustomXML-directory
Proceed as you normally would with a blank template

Note that this is might not be supported, but seems to work just fine.
]]></content>
      <tags>
        <tag>Dynamics 365</tag>
      </tags>
  </entry>
  <entry>
    <title>Export Data from Dynamics 365 using PowerShell</title>
    <url>/2017/07/10/Export-Data-from-Dynamics-365-using-PowerShell/</url>
    <content><![CDATA[Did you know that you can manage your Dynamics 365 instance using your favorite automation tool PowerShell? The Microsoft.Xrm.Data.PowerShell-module lets you manage your solutions and such but also has functionality to work on records. In this post I am going to show you how to use the module for data export.
The first thing you need to do is install the module on your system. Just run the command Install-Module Microsoft.Xrm.Data.PowerShell in PowerShell to get it. Next you need to connect to your instance. There are a few alternatives here so please refer to the project’s GitHub-page. With the Microsoft hosted Dynamics 365 you can use Connect-CrmOnlineDiscovery -InteractiveModeto connect using a GUI.
Working with dataOnce connected you can try querying for some data. The command Get-CrmRecords -EntityLogicalName account gets you all (or the first 5000) accounts in your system. The output looks something like this:
Key          Value---          -----CrmRecords   &#123;@&#123;accountid=f39551c1-81ff-e611-80f3-5065f38a5a01; accountid_Property=[ac...Count        349PagingCookie &lt;cookie page=&quot;1&quot;&gt;&lt;accountid last=&quot;&#123;9AD008BF-8A34-E711-80FA-5065F38BC5C1&#125;&quot;...NextPage     FalseFetchXml         &lt;fetch version=&quot;1.0&quot; output-format=&quot;xml-platform&quot; mapping=&quot;logical&quot; d...

As you can see the result is an object with a few different properties. The CrmRecords property contains the actual result of the query. So if you want to output all the accounts you would run (Get-CrmRecords -EntityLogicalName account).CrmRecords. That gets you something like this:
accountid                 : f39551c1-81ff-e611-80f3-5065f38a5a01accountid_Property        : [accountid, f39551c1-81ff-e611-80f3-5065f38a5a01]ReturnProperty_EntityName : accountReturnProperty_Id         : f39551c1-81ff-e611-80f3-5065f38a5a01original                  : &#123;[accountid_Property, [accountid, f39551c1-81ff-e611-80f3-506                            5f38a5a01]], [accountid, f39551c1-81ff-e611-80f3-5065f38a5a01                            ], [ReturnProperty_EntityName, account], [ReturnProperty_Id ,                             f39551c1-81ff-e611-80f3-5065f38a5a01]&#125;logicalname               : accountaccountid_Property        : [accountid, b902322e-6005-e711-80f4-5065f38a5a01]accountid                 : b902322e-6005-e711-80f4-5065f38a5a01ReturnProperty_EntityName : accountReturnProperty_Id         : b902322e-6005-e711-80f4-5065f38a5a01original                  : &#123;[accountid_Property, [accountid, b902322e-6005-e711-80f4-506                            5f38a5a01]], [accountid, b902322e-6005-e711-80f4-5065f38a5a01                            ], [ReturnProperty_EntityName, account], [ReturnProperty_Id ,                             b902322e-6005-e711-80f4-5065f38a5a01]&#125;logicalname               : account

As you can see there doesn’t seem to be any useful data other than the Id to work with. That’s because we haven’t requested any attributes. To include the account name you need to use the -Fields parameter. Eg. Get-CrmRecords -EntityLogicalName account -Fields &quot;name&quot; includes the account name in the result like so:
name                      : Test accountname_Property             : [name, Test account]accountid_Property        : [accountid, f39551c1-81ff-e611-80f3-5065f38a5a01]accountid                 : f39551c1-81ff-e611-80f3-5065f38a5a01ReturnProperty_EntityName : accountReturnProperty_Id         : f39551c1-81ff-e611-80f3-5065f38a5a01original                  : &#123;[name_Property, [name, Test account]], [name, Test                            account], [accountid_Property, [accountid, f39551c1-81ff                            -e611-80f3-5065f38a5a01]], [accountid, f39551c1-81ff-e611-80f                            3-5065f38a5a01]...&#125;logicalname               : account

So now that we know how to get data from the system, let’s get it into a CSV-file. You can’t quite do a Get-CrmRecords | Export-CSV, but close to it. Here’s a code snippet:
(Get-CrmRecords -EntityLogicalName account -Fields &quot;name&quot;,&quot;telephone1&quot;).CrmRecords |    Select-Object -Property &quot;name&quot;,&quot;telephone1&quot; |    Export-Csv -Path &quot;accounts.csv&quot;

The code fetches all accounts from the system, selects just the properties we want and finally exports the result to a CSV-file. You might want to use the -NoTypeInformation, -Delimeter and -Encoding parameters. You can see the final code with some improvements here.
In my next post we will improve on this by supplying our own column names instead of the schema names provided by the system.
]]></content>
      <tags>
        <tag>PowerShell</tag>
        <tag>Dynamics 365</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Ben Thompson - Microsoft and Software Survival</title>
    <url>/2026/02/07/ben-thompson-microsoft-and-software-survival/</url>
    <content><![CDATA[Ben Thompson in the article Microsoft and Software Survival (published February 3, 2026):

That, then, raises the most obvious bear case for any software company: why pay for software when you can just ask AI to write your own application, perfectly suited to your needs? Is software going to be a total commodity and a non-viable business model in the future?
I’m skeptical, for a number of reasons. First, companies — particularly American ones — are very good at focusing on their core competency, and for most companies in the world, that isn’t software. There is a reason most companies pay other companies for software, and the most fundamental reason to do so won’t change with AI.
Second, writing the original app is just the beginning: there is maintenance, there are security patches, there are new features, there are changing standards — writing an app is a commitment to a never-ending journey — a journey, to return to point one, that has nothing to do with the company’s core competency.
Third, selling software isn’t just about selling code. There is support, there is compliance, there are integrations with other software, the list of what is actually valuable goes far beyond code. This is why companies don’t run purely open source software: they don’t want code, they want a product, with everything that entails.

As the cost of developing sofware, at least the coding bit, approaches zero other concerns are more pressing.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Generate CSV from Dynamics 365 with Custom Headers</title>
    <url>/2017/07/11/Generate-CSV-from-Dynamics-365-with-Custom-Headers/</url>
    <content><![CDATA[In my previous post we retrieved account records from Dynamics 365 (CRM) and exported those to a CSV-file. In this post we will expand the example with a transformation function to get nicer property names and CSV headers.
The Export-CSV cmdlet exports all the attributes on the objects piped to it. In our last example we used Select-Object to only include the properties we wanted. These properties are still named by the original object. In most cases this is not desirable. You could just replace the CSV file header row, but a cleaner way would be to transform the object somehow. One way to achieve this is to use Select-Object as mentioned here.
Using our example from before:
(Get-CrmRecords -EntityLogicalName account -Fields &quot;name&quot;,&quot;telephone1&quot;).CrmRecords |Select-Object @&#123;        N=&quot;AccountName&quot;        E=&#123;$_.name&#125;    &#125;,    @&#123;        N=&quot;Phonenumber&quot;        E=&#123;$_.telephone1&#125;    &#125; |Export-Csv -Path &quot;accounts.csv&quot;
This is a nice trick and certainly very flexible. You can select any property on the source object and assign it to a new property. But what if you want to parametrize and use a mapping table for the transformation? You could probably cook-up a function that provides a mapping array for Select-Object. But since I only just came up with that and it actually sounds a bit complicated, I propose a different approach.
Let’s create an advanced function to do the heavy lifting for us:
function ConvertTo-CustomObject&#123;    [CmdletBinding()]    param(        [Parameter(Mandatory=$True,ValueFromPipeline=$True)]        $SourceObject,        [Parameter(Mandatory=$True)]        [hashtable]$MappingTable    )    PROCESS &#123;        $Property = @&#123;&#125;        $MappingTable.Keys | ForEach-Object &#123;            $Property.Add(                $MappingTable[$_],                $SourceObject.$_            )        &#125;        New-Object psobject -Property $Property    &#125;&#125;
The function ConvertTo-CustomObject accepts objects from the pipeline and a mapping table. It then uses the mapping table to iterate across the the properties we want adding them to a property hashtable. Lastly it creates a new object using the properties and writes it to the output.
So again, using our example, we create a mapping table like so:
$FieldsMappings = @&#123;    &quot;name&quot; = &quot;AccountName&quot;    &quot;telephone1&quot; = &quot;Phone&quot;&#125;
And then we can replace the Select-Object part with our function:
(Get-CrmRecords -EntityLogicalName account -Fields $FieldsMappings.Keys -AllRows).CrmRecords |    ConvertTo-CustomObject -MappingTable $FieldsMappings |    Export-Csv -Path &quot;account.csv&quot;
We retrieve the attributes we want from the system (the keys of the $Fieldmappings hashtable), pipe the objects to our function and write the resulting objects to a CSV like before, but with a nicer header row. Complete code here.
While this may seem like a complex way to do it, using objects gets us some nice benefits. Eg. try replacing the Export-CSV line with Out-Gridview and see that happens. If you would like to include more attributes, all you need to is add items to the mapping table.
]]></content>
      <tags>
        <tag>PowerShell</tag>
        <tag>Dynamics 365</tag>
      </tags>
  </entry>
  <entry>
    <title>My new shiny Markdown compatible blog</title>
    <url>/2017/04/25/How-this-blog-was-made/</url>
    <content><![CDATA[MarkdownI remember hearing about Markdown previously but hadn’t paid it much attention. Then I heard it mentioned on a podcast I listen to  I also noticed that the Readme file on Visual Studio Team Services was done in markdown.


Naturally I was intrigued. As a wannabe programmer and configuration nazi the idea of using a simple markup syntax to write formatted documents sounded great. I had a brief stint using LaTeX while at school (no one else did though) and while I did do my final year project documentation with it, I quickly lost interest.
The blogAnyway… I had thought about starting a blog for a while. I use Feedly every day and follow lots of blogs. Since I’ve benefited greatly from the community I thought it was time to give something back.
Getting back to the main topic of how this blog came about. I searched for blogging engines that supported Markdown and the first result on Google was Hexo. Hexo is a static site generator built using NodeJS. Seemed like a great fit for a backend guy like me.
I installed NodeJS using Chocolatey and followed the instructions to get up and running. Simple enough. The original plan was to host it on an Azure Web App, so I followed another set of instructions in order to do that.
Note that in the Azure portal you should use the deployment options instead of continuous integration. That’s where you find the Local Git repo option you need. I first tried the Free plan, but apparently custom domains are not supported on that, so I switched to the Shared plan. See more details here. I’ll probably move the blog to Github pages though since that’s free. Or I could host it on my router…
Once everything was setup all I need to do to update the blog, besides writing the posts, is hexo deploy --generate. I placed the blog source files in Dropbox so I can get to them wherever I am. This post was started on my phone and finished on an iPad. I can’t generate and deploy on the go though. I’m sure PowerShell can help, but that is another blog post I think. I’m thinking some kind of change monitoring and automated deployment.
Please note that I’m using this blog as another learning opportunity, so except changes, downtime etc. I’m also not sure how I active I’m actually going to be.
]]></content>
      <tags>
        <tag>Hexo</tag>
        <tag>NodeJS</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Steve Yegge - Stevey’s Birthday Blog</title>
    <url>/2026/02/07/steve-yegge-steveys-birthday-blog/</url>
    <content><![CDATA[Steve Yegge in the article Stevey’s Birthday Blog (published February 2, 2026):

Agent orchestrators are programs that use agents to run other agents.
There are, as far as I can tell, four main players in this space: Ralph Wiggum, Loom, Claude Flow, and Gas Town. More will come very soon, from all corners. But I thought I’d share my thoughts on how to tell the current group apart, as they are all very important.
[…]
I think Geoffrey Huntley and I have been exploring two of the key components of the Industrial Revolution happening in agentic software development: factories, and workers. In my view, Claude Code is a worker. Geoffrey has built a super-worker, and also a team of super workers with Loom, whereas I’ve built a super-factory. These are all orthogonal, complementary, and necessary.
Claude Flow is a bit different; it also tries to be a factory, and it’s quite clever, but it’s not thinking about the problem the way I am. All three of the others have orchestration as their core primitive. Whereas for Gas Town, the work itself is the primitive. Gas Town federates work into an auditable ledger for tracking millions of work items in a blockchain. I’m solving a completely different problem.

I’ve been reading Steve’s and Gene Kim’s Vibe Coding book and it’s a super interesting journey Steve is on. Where he leads others will follow.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI Coding</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Neeraj Nandwana - Generally available: host and run code apps in Power Apps</title>
    <url>/2026/02/11/neeraj-nandwana-generally-available-host-and-run-code-apps-in-power-apps/</url>
    <content><![CDATA[Neeraj Nandwana in the article Generally available: host and run code apps in Power Apps (published February 9, 2026):

Power Apps code apps bring the full strength of Power Platform to web developers. Build in any code‑first IDE, iterate locally, and run the same app seamlessly within Power Platform. Build with popular frameworks (React, Vue, and others) while keeping full control over your UI and logic.

One aspect that is interesting with this technology is that it’s “normal web development” and the data sources are neatly wrapped as service providers. Since you make your own UI you can, by making smart decisions, keep the app loosely coupled to the backend.
If you later decide you want to transition away from Power Platform, you only need to implement the backend again making the transition easier for your users. There’s obviously a lot of benefits you get from Power Platform, but you might not need all of it.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI Coding</tag>
        <tag>Power Apps</tag>
      </tags>
  </entry>
</search>
