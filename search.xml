<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Automating Quote Post Creation with Feedly and Hexo</title>
    <url>/2026/02/06/Automating-Quote-Posts-with-Feedly-and-Hexo/</url>
    <content><![CDATA[I spend a lot of my time reading blog articles using my feed reader and wanted a lightweight way to capture interesting quotes and commentary directly into my blog. Rather than manually creating markdown files, I built, with a lot of assistance from Claude, a two-part automation system: a Node.js script that generates posts from parameters, and a browser bookmarklet that extracts article data from Feedly.
The basic idea is that when I read through my feed and see something interesting I can use Feedly’s native highlighting functionality and notes to add my commentary and then use that as a source for post generation.
How Hexo WorksI currently use Hexo for my blog. Hexo is a static site generator that converts markdown files into HTML. Here’s the basic workflow:

Source files: You write markdown files in source/_posts/ with YAML frontmatter metadata (title, date, tags, etc.)
Generation: Running hexo generate processes these files through templates and outputs static HTML to the public/ directory
Deployment: hexo deploy pushes the generated HTML to your hosting (in my case, GitHub Pages)

The key advantage is that your entire blog is version-controlled markdown files—no database, no complex tooling. Each post is just a file with metadata and content. I also do not need to worry about anyone hacking my site as there is no administrator login.
Creating the Quote Post ScriptTo automate post creation based on the article. I built a Node.js CLI script (tools/create-quote-post.js) that:

Accepts command-line parameters: author, URL, title, publication date, quote, and commentary
Generates proper frontmatter: Creates YAML metadata with tags, source information, and post date (today, not the article date)
Formats the markdown body: Renders the quote as a blockquote and includes your commentary
Writes the file: Creates a markdown file in source/_posts/ with a sanitized filename

Examplenode tools/create-quote-post.js \  --author &quot;Ben Thompson&quot; \  --url &quot;https://stratechery.com/...&quot; \  --title &quot;Microsoft and Software Survival&quot; \  --date &quot;2026-02-07&quot; \  --quote &quot;That, then, raises the most obvious bear case...&quot; \  --commentary &quot;Really interesting perspective&quot;

Generates a post with:

Frontmatter including source_author, source_url, and source_date (preserving the original article’s date)
Post date set to today
Proper YAML list formatting for tags
Quote rendered as markdown blockquote

Directory Structure: The scripts live in tools/ rather than Hexo’s scripts/ directory, which prevents Hexo from trying to load them as plugins during the build process.
This script serves as the base for the automation hook.
Extracting Data with a Feedly BookmarkletFor extracting the data off of Feedly there are a few options. Feedly does have an API but that is only available for the Enterprise plan. One option to create a workflow would have been to use IFTTT to create an automation using the Feedly applets and then call a web api with that. I might still do that as that might more mobile friendly, but for a first try I decided to just use the browser. As a simpler alternative to a browser addon, I made a bookmarklet that:

Extracts article data from Feedly’s DOM:
Title, URL, author&#x2F;publication name
Publication date (parsed from Feedly’s date format)
All highlighted text (grouped by paragraph to preserve structure)
Any notes&#x2F;comments I added in Feedly
Feedly board names as tags





Presents an editable form in a modal dialog where I can:
Review and edit all extracted data
Add or modify my comments further
Adjust tags before generating





Generates the bash command for the post creation, ready to copy and paste into my terminal

Part 2: Web Server Automation (Coming Soon)While the bookmarklet works well for one-off posts, the next step is building a lightweight web server that can:

Accept article data via HTTP (from a bookmarklet or API endpoint)
Generate and commit the post automatically
Trigger a blog rebuild and deployment
Return the published post URL

This would eliminate the need to manually run the script, enabling a one-click publishing workflow directly from Feedly.

Want to try it? The bookmarklet and script are open for customization. You can find:

The extraction logic and minified version in feedly-bookmarklet.js
The post creation script in create-quote-post.js

Note: At the moment the form is not quite usable inside Feedly, but the buttons work.
]]></content>
      <tags>
        <tag>Hexo</tag>
        <tag>NodeJS</tag>
        <tag>Automation</tag>
        <tag>Feedly</tag>
        <tag>Bookmarklet</tag>
      </tags>
  </entry>
  <entry>
    <title>ConvertTo-CustomObject function as a wrapper for Select-Object</title>
    <url>/2017/07/12/ConvertTo-CustomObject-function-as-a-wrapper-for-Select-Object/</url>
    <content><![CDATA[In yesterday’s post I introduced a custom advanced function for changing property names on objects. I also showed you how to use Select-Object to do the same. Because the Select-Object cmdlet is probably more robust than my little function I decided to try and rewrite the function to use Select-Object under the hood.
Select-Object accepts an array of hashtable objects with two key&#x2F;value pairs:

N is for the name of the property
E is the expression as a script block that need to be run to get the value

In order to use this functionality we need to figure out a way to generate these objects dynamically using our mapping table. The approach I took was to generate the code as strings and then use Invoke-Expression. Since we only need to do this once and not for every object I added a BEGIN block to my function:
BEGIN &#123;    $Expression = &quot;&quot;    $MappingTable.Keys | ForEach-Object &#123;        $Expression += (            @&quot;                @&#123;                    N=&quot;$($MappingTable[$_])&quot;                    E=&#123;`$_.$_&#125;                &#125;,        &quot;@        )    &#125;    $Expression = $Expression.Substring(0,$Expression.Length-1)    $SelectObjectParameter = Invoke-Expression $Expression&#125;
So for each key in the mapping table we generate a name and an expression to run. Eg. using a mapping table like @{ &quot;name&quot; = &quot;AccountName&quot;; &quot;telephone1&quot; = &quot;Phone&quot;  }generates the following string:
@&#123;    N=&quot;Phone&quot;    E=&#123;$_.telephone1&#125;&#125;,                    @&#123;    N=&quot;AccountName&quot;    E=&#123;$_.name&#125;&#125;
That string is exactly what we would feed Select-Object. To get it into actual PowerShell objects we need to feed it to Invoke-Expression. That gives us an array with two separate hashtables each with a N key with a string value and an E key with a script block value.
Once stored in a variable we can use this for each of the objects fed through the pipeline by using it in the PROCESS block of our advanced function:
PROCESS &#123;        $SourceObject | Select-Object -Property $SelectObjectParameter    &#125;  
And that is all there is to it. The function works exactly the same as the ‘quick and dirty’ one in the last post. You can find the full function here. Import it to your session and try this example: Get-Process | ConvertTo-CustomObject -MappingTable @{ &quot;ProcessName&quot; = &quot;Name&quot; }
]]></content>
      <tags>
        <tag>PowerShell</tag>
      </tags>
  </entry>
  <entry>
    <title>Blog moved to Github Pages</title>
    <url>/2017/06/11/Blog-moved-to-Github-Pages/</url>
    <content><![CDATA[Just a quick post to see that the blog publishing still works. I moved the blog from Azure to Github pages. All in all really simple. You just create a repository and publish via git. In my case as I was already using git for publishing all I needed to do was change the git endpoint, add a custom domain and point my DNS to github.
]]></content>
  </entry>
  <entry>
    <title>Dynamics 365 Document Template from Existing Word Document</title>
    <url>/2017/04/23/Dynamics-365-Document-Template-from-existing-Word-Document/</url>
    <content><![CDATA[For the impatient this is how you do it:

Extract the Dynamics 365 generated template from zip and add the CustomXML-part to your existing template.

I couldn’t find a post on how to create a new Dynamics 365 Document Template using an existing document or template. I’m sure many organisations have an existing template with all the right formatting, headers and footers they would like to use.
Knowing how .docx-files are structured and how Dynamics 365 stores the metadata it was easy enough to figure out. I thought I would share how with you.
Step by step
Create a new blank template from Dynamics 365 the way you would normally do it and save the file to your computer
Change the extension of the file to .zip and extract it
Open the existing document you would like to use with Dynamics 365
In your Word template click Developer &gt; XML Mapping Pane
In the drop down menu select (Add new part…)
Navigate to your extracted template and select the xml-file (item.xml) under the CustomXML-directory
Proceed as you normally would with a blank template

Note that this is might not be supported, but seems to work just fine.
]]></content>
      <tags>
        <tag>Dynamics 365</tag>
      </tags>
  </entry>
  <entry>
    <title>Export Data from Dynamics 365 using PowerShell</title>
    <url>/2017/07/10/Export-Data-from-Dynamics-365-using-PowerShell/</url>
    <content><![CDATA[Did you know that you can manage your Dynamics 365 instance using your favorite automation tool PowerShell? The Microsoft.Xrm.Data.PowerShell-module lets you manage your solutions and such but also has functionality to work on records. In this post I am going to show you how to use the module for data export.
The first thing you need to do is install the module on your system. Just run the command Install-Module Microsoft.Xrm.Data.PowerShell in PowerShell to get it. Next you need to connect to your instance. There are a few alternatives here so please refer to the project’s GitHub-page. With the Microsoft hosted Dynamics 365 you can use Connect-CrmOnlineDiscovery -InteractiveModeto connect using a GUI.
Working with dataOnce connected you can try querying for some data. The command Get-CrmRecords -EntityLogicalName account gets you all (or the first 5000) accounts in your system. The output looks something like this:
Key          Value---          -----CrmRecords   &#123;@&#123;accountid=f39551c1-81ff-e611-80f3-5065f38a5a01; accountid_Property=[ac...Count        349PagingCookie &lt;cookie page=&quot;1&quot;&gt;&lt;accountid last=&quot;&#123;9AD008BF-8A34-E711-80FA-5065F38BC5C1&#125;&quot;...NextPage     FalseFetchXml         &lt;fetch version=&quot;1.0&quot; output-format=&quot;xml-platform&quot; mapping=&quot;logical&quot; d...

As you can see the result is an object with a few different properties. The CrmRecords property contains the actual result of the query. So if you want to output all the accounts you would run (Get-CrmRecords -EntityLogicalName account).CrmRecords. That gets you something like this:
accountid                 : f39551c1-81ff-e611-80f3-5065f38a5a01accountid_Property        : [accountid, f39551c1-81ff-e611-80f3-5065f38a5a01]ReturnProperty_EntityName : accountReturnProperty_Id         : f39551c1-81ff-e611-80f3-5065f38a5a01original                  : &#123;[accountid_Property, [accountid, f39551c1-81ff-e611-80f3-506                            5f38a5a01]], [accountid, f39551c1-81ff-e611-80f3-5065f38a5a01                            ], [ReturnProperty_EntityName, account], [ReturnProperty_Id ,                             f39551c1-81ff-e611-80f3-5065f38a5a01]&#125;logicalname               : accountaccountid_Property        : [accountid, b902322e-6005-e711-80f4-5065f38a5a01]accountid                 : b902322e-6005-e711-80f4-5065f38a5a01ReturnProperty_EntityName : accountReturnProperty_Id         : b902322e-6005-e711-80f4-5065f38a5a01original                  : &#123;[accountid_Property, [accountid, b902322e-6005-e711-80f4-506                            5f38a5a01]], [accountid, b902322e-6005-e711-80f4-5065f38a5a01                            ], [ReturnProperty_EntityName, account], [ReturnProperty_Id ,                             b902322e-6005-e711-80f4-5065f38a5a01]&#125;logicalname               : account

As you can see there doesn’t seem to be any useful data other than the Id to work with. That’s because we haven’t requested any attributes. To include the account name you need to use the -Fields parameter. Eg. Get-CrmRecords -EntityLogicalName account -Fields &quot;name&quot; includes the account name in the result like so:
name                      : Test accountname_Property             : [name, Test account]accountid_Property        : [accountid, f39551c1-81ff-e611-80f3-5065f38a5a01]accountid                 : f39551c1-81ff-e611-80f3-5065f38a5a01ReturnProperty_EntityName : accountReturnProperty_Id         : f39551c1-81ff-e611-80f3-5065f38a5a01original                  : &#123;[name_Property, [name, Test account]], [name, Test                            account], [accountid_Property, [accountid, f39551c1-81ff                            -e611-80f3-5065f38a5a01]], [accountid, f39551c1-81ff-e611-80f                            3-5065f38a5a01]...&#125;logicalname               : account

So now that we know how to get data from the system, let’s get it into a CSV-file. You can’t quite do a Get-CrmRecords | Export-CSV, but close to it. Here’s a code snippet:
(Get-CrmRecords -EntityLogicalName account -Fields &quot;name&quot;,&quot;telephone1&quot;).CrmRecords |    Select-Object -Property &quot;name&quot;,&quot;telephone1&quot; |    Export-Csv -Path &quot;accounts.csv&quot;

The code fetches all accounts from the system, selects just the properties we want and finally exports the result to a CSV-file. You might want to use the -NoTypeInformation, -Delimeter and -Encoding parameters. You can see the final code with some improvements here.
In my next post we will improve on this by supplying our own column names instead of the schema names provided by the system.
]]></content>
      <tags>
        <tag>PowerShell</tag>
        <tag>Dynamics 365</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Ben Thompson - Microsoft and Software Survival</title>
    <url>/2026/02/06/ben-thompson-microsoft-and-software-survival/</url>
    <content><![CDATA[Ben Thompson in the article Microsoft and Software Survival (published February 3, 2026):

That, then, raises the most obvious bear case for any software company: why pay for software when you can just ask AI to write your own application, perfectly suited to your needs? Is software going to be a total commodity and a non-viable business model in the future?
I’m skeptical, for a number of reasons. First, companies — particularly American ones — are very good at focusing on their core competency, and for most companies in the world, that isn’t software. There is a reason most companies pay other companies for software, and the most fundamental reason to do so won’t change with AI.
Second, writing the original app is just the beginning: there is maintenance, there are security patches, there are new features, there are changing standards — writing an app is a commitment to a never-ending journey — a journey, to return to point one, that has nothing to do with the company’s core competency.
Third, selling software isn’t just about selling code. There is support, there is compliance, there are integrations with other software, the list of what is actually valuable goes far beyond code. This is why companies don’t run purely open source software: they don’t want code, they want a product, with everything that entails.

As the cost of developing sofware, at least the coding bit, approaches zero other concerns are more pressing.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Generate CSV from Dynamics 365 with Custom Headers</title>
    <url>/2017/07/11/Generate-CSV-from-Dynamics-365-with-Custom-Headers/</url>
    <content><![CDATA[In my previous post we retrieved account records from Dynamics 365 (CRM) and exported those to a CSV-file. In this post we will expand the example with a transformation function to get nicer property names and CSV headers.
The Export-CSV cmdlet exports all the attributes on the objects piped to it. In our last example we used Select-Object to only include the properties we wanted. These properties are still named by the original object. In most cases this is not desirable. You could just replace the CSV file header row, but a cleaner way would be to transform the object somehow. One way to achieve this is to use Select-Object as mentioned here.
Using our example from before:
(Get-CrmRecords -EntityLogicalName account -Fields &quot;name&quot;,&quot;telephone1&quot;).CrmRecords |Select-Object @&#123;        N=&quot;AccountName&quot;        E=&#123;$_.name&#125;    &#125;,    @&#123;        N=&quot;Phonenumber&quot;        E=&#123;$_.telephone1&#125;    &#125; |Export-Csv -Path &quot;accounts.csv&quot;
This is a nice trick and certainly very flexible. You can select any property on the source object and assign it to a new property. But what if you want to parametrize and use a mapping table for the transformation? You could probably cook-up a function that provides a mapping array for Select-Object. But since I only just came up with that and it actually sounds a bit complicated, I propose a different approach.
Let’s create an advanced function to do the heavy lifting for us:
function ConvertTo-CustomObject&#123;    [CmdletBinding()]    param(        [Parameter(Mandatory=$True,ValueFromPipeline=$True)]        $SourceObject,        [Parameter(Mandatory=$True)]        [hashtable]$MappingTable    )    PROCESS &#123;        $Property = @&#123;&#125;        $MappingTable.Keys | ForEach-Object &#123;            $Property.Add(                $MappingTable[$_],                $SourceObject.$_            )        &#125;        New-Object psobject -Property $Property    &#125;&#125;
The function ConvertTo-CustomObject accepts objects from the pipeline and a mapping table. It then uses the mapping table to iterate across the the properties we want adding them to a property hashtable. Lastly it creates a new object using the properties and writes it to the output.
So again, using our example, we create a mapping table like so:
$FieldsMappings = @&#123;    &quot;name&quot; = &quot;AccountName&quot;    &quot;telephone1&quot; = &quot;Phone&quot;&#125;
And then we can replace the Select-Object part with our function:
(Get-CrmRecords -EntityLogicalName account -Fields $FieldsMappings.Keys -AllRows).CrmRecords |    ConvertTo-CustomObject -MappingTable $FieldsMappings |    Export-Csv -Path &quot;account.csv&quot;
We retrieve the attributes we want from the system (the keys of the $Fieldmappings hashtable), pipe the objects to our function and write the resulting objects to a CSV like before, but with a nicer header row. Complete code here.
While this may seem like a complex way to do it, using objects gets us some nice benefits. Eg. try replacing the Export-CSV line with Out-Gridview and see that happens. If you would like to include more attributes, all you need to is add items to the mapping table.
]]></content>
      <tags>
        <tag>PowerShell</tag>
        <tag>Dynamics 365</tag>
      </tags>
  </entry>
  <entry>
    <title>My new shiny Markdown compatible blog</title>
    <url>/2017/04/25/How-this-blog-was-made/</url>
    <content><![CDATA[MarkdownI remember hearing about Markdown previously but hadn’t paid it much attention. Then I heard it mentioned on a podcast I listen to  I also noticed that the Readme file on Visual Studio Team Services was done in markdown.


Naturally I was intrigued. As a wannabe programmer and configuration nazi the idea of using a simple markup syntax to write formatted documents sounded great. I had a brief stint using LaTeX while at school (no one else did though) and while I did do my final year project documentation with it, I quickly lost interest.
The blogAnyway… I had thought about starting a blog for a while. I use Feedly every day and follow lots of blogs. Since I’ve benefited greatly from the community I thought it was time to give something back.
Getting back to the main topic of how this blog came about. I searched for blogging engines that supported Markdown and the first result on Google was Hexo. Hexo is a static site generator built using NodeJS. Seemed like a great fit for a backend guy like me.
I installed NodeJS using Chocolatey and followed the instructions to get up and running. Simple enough. The original plan was to host it on an Azure Web App, so I followed another set of instructions in order to do that.
Note that in the Azure portal you should use the deployment options instead of continuous integration. That’s where you find the Local Git repo option you need. I first tried the Free plan, but apparently custom domains are not supported on that, so I switched to the Shared plan. See more details here. I’ll probably move the blog to Github pages though since that’s free. Or I could host it on my router…
Once everything was setup all I need to do to update the blog, besides writing the posts, is hexo deploy --generate. I placed the blog source files in Dropbox so I can get to them wherever I am. This post was started on my phone and finished on an iPad. I can’t generate and deploy on the go though. I’m sure PowerShell can help, but that is another blog post I think. I’m thinking some kind of change monitoring and automated deployment.
Please note that I’m using this blog as another learning opportunity, so except changes, downtime etc. I’m also not sure how I active I’m actually going to be.
]]></content>
      <tags>
        <tag>Hexo</tag>
        <tag>NodeJS</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Steve Yegge - Stevey’s Birthday Blog</title>
    <url>/2026/02/06/steve-yegge-steveys-birthday-blog/</url>
    <content><![CDATA[Steve Yegge in the article Stevey’s Birthday Blog (published February 2, 2026):

Agent orchestrators are programs that use agents to run other agents.
There are, as far as I can tell, four main players in this space: Ralph Wiggum, Loom, Claude Flow, and Gas Town. More will come very soon, from all corners. But I thought I’d share my thoughts on how to tell the current group apart, as they are all very important.
[…]
I think Geoffrey Huntley and I have been exploring two of the key components of the Industrial Revolution happening in agentic software development: factories, and workers. In my view, Claude Code is a worker. Geoffrey has built a super-worker, and also a team of super workers with Loom, whereas I’ve built a super-factory. These are all orthogonal, complementary, and necessary.
Claude Flow is a bit different; it also tries to be a factory, and it’s quite clever, but it’s not thinking about the problem the way I am. All three of the others have orchestration as their core primitive. Whereas for Gas Town, the work itself is the primitive. Gas Town federates work into an auditable ledger for tracking millions of work items in a blockchain. I’m solving a completely different problem.

I’ve been reading Steve’s and Gene Kim’s Vibe Coding book and it’s a super interesting journey Steve is on. Where he leads others will follow.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI Coding</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Neeraj Nandwana - Generally available: host and run code apps in Power Apps</title>
    <url>/2026/02/10/neeraj-nandwana-generally-available-host-and-run-code-apps-in-power-apps/</url>
    <content><![CDATA[Neeraj Nandwana in the article Generally available: host and run code apps in Power Apps (published February 9, 2026):

Power Apps code apps bring the full strength of Power Platform to web developers. Build in any code‑first IDE, iterate locally, and run the same app seamlessly within Power Platform. Build with popular frameworks (React, Vue, and others) while keeping full control over your UI and logic.

One aspect that is interesting with this technology is that it’s “normal web development” and the data sources are neatly wrapped as service providers. Since you make your own UI you can, by making smart decisions, keep the app loosely coupled to the backend.
If you later decide you want to transition away from Power Platform, you only need to implement the backend again making the transition easier for your users. There’s obviously a lot of benefits you get from Power Platform, but you might not need all of it.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI Coding</tag>
        <tag>Power Apps</tag>
        <tag>Power Platform</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Neeraj Nandwana - Announcing General Availability (GA) of building single-page applications for Power Pages</title>
    <url>/2026/02/10/neeraj-nandwana-announcing-general-availability-ga-of-building-single-page-applications-for-power-pages/</url>
    <content><![CDATA[Neeraj Nandwana in the article Announcing General Availability (GA) of building single-page applications for Power Pages (published February 9, 2026):

You can run your SPA locally with full authentication and Web API access to accelerate your development cycle. This means JavaScript hot reload, local debugging, and immediate feedback without deploying changes to your portal every time. For step-by-step instructions, see Set up local development for SPA sites.

When you want to authenticate locally you need to login to the live portal once using the built in Entra ID identity provider. That makes sure you have a contact that is connected to your Entra user. When you login on localhost that is the identty you use with the development proxy.
As far as I can tell you cannot currently use another provider this, but I might be wrong. The documentation is currently not very broad.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI Coding</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting codemanship - An Ode To “It Can’t Be Done”</title>
    <url>/2026/02/11/codemanship-an-ode-to-it-cant-be-done/</url>
    <content><![CDATA[codemanship in the article An Ode To “It Can’t Be Done” (published February 10, 2026):

Command-and-control cultures are often a product of lack of trust. And in many cases, that lack of trust has been earned by past performance. Many disappointments, many broken promises. So organisations micromanage, and if anything’s guaranteed to end an experiment in software agility, it’s that.
To earn trust, teams need to deliver rapidly, reliably and sustainably. To gain the autonomy needed to deliver rapidly, reliably and sustainable, teams need to be trusted. Catch 22.
[…]
And the poker metaphor is very appropriate here, since the real benefit of agile software development is minimising risk. We don’t let uncertainty pile up into big batches and big releases, and managers usually realise that their picture of actual progress is far more realistic, enabling them to make more informed decisions grounded in the reality of user feedback about working software.

Trust brings freedom.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>Software Development</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Unknown - Introducing Showboat and Rodney, so agents can demo what they’ve built</title>
    <url>/2026/02/11/unknown-introducing-showboat-and-rodney-so-agents-can-demo-what-theyve-built/</url>
    <content><![CDATA[Simon Willison in the article Introducing Showboat and Rodney, so agents can demo what they’ve built (published February 10, 2026):

The frontier models all understand that “red&#x2F;green TDD” means they should write the test first, run it and watch it fail and then write the code to make it pass - it’s a convenient shortcut.
I find this greatly increases the quality of the code and the likelihood that the agent will produce the right thing with the smallest amount of prompts to guide it.

]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI Coding</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Scott Hanselman - Is the craft dead?</title>
    <url>/2026/02/12/scott-hanselman-is-the-craft-dead/</url>
    <content><![CDATA[Scott Hanselman in the article Is the craft dead? (published January 17, 2026):

I’ve been coding now for money for 35 years and systems are still complicated, computers still do dumb stuff, humans still do dumb stuff,
There is value in good taste, there is value in craftsmanship, and there is value in human judgment.
[…]
I think that there will be lots of work for us cleaning up after the slop, but if you know what you’re doing AI augmented development is going to get you some amazing results and I am enjoying learning a ton during this momentous era shift - but the craft still exists.

I agree. Getting quality results from LLMs requires discipline.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI Coding</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Geoffrey Huntley - don’t waste your back pressure</title>
    <url>/2026/02/12/geoffrey-huntley-dont-waste-your-back-pressure/</url>
    <content><![CDATA[Geoffrey Huntley in the article don’t waste your back pressure (published January 17, 2026):

There are many ways to tune back-pressure and as Moss states it starts with choice of programming language, applying engineering knowledge to design a fast test suite that provides signal but perhaps my favorite one is… pre-commit hooks (aka prek).

]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI Coding</tag>
      </tags>
  </entry>
  <entry>
    <title>Extending Quote Post Automation: From Feedly-Specific to Universal Meta Tag Extraction</title>
    <url>/2026/02/12/Extending-Quote-Post-Automation-with-Meta-Tags/</url>
    <content><![CDATA[In my previous post, I documented a two-part system for automating quote post creation: a Node.js script and a Feedly-specific bookmarklet. Since then, I’ve extended the tooling to be more general-purpose and robust.
The Problem with Feedly-Only ExtractionThe original bookmarklet relied on Feedly’s specific DOM structure, which meant it only worked when reading articles within Feedly’s interface. This was limiting—I wanted to capture quotes from:

Articles linked in tweets
Blog posts I encounter through search
News sites I visit directly
Any webpage with proper meta tags

The Solution: Universal Meta Tag ExtractionInstead of parsing Feedly’s DOM, I refactored the bookmarklet to extract metadata using standard HTML meta tags that most modern websites include:
&lt;meta property=&quot;og:author&quot; content=&quot;Author Name&quot;&gt;&lt;meta property=&quot;og:title&quot; content=&quot;Article Title&quot;&gt;&lt;meta property=&quot;article:published_time&quot; content=&quot;2026-02-07&quot;&gt;&lt;meta property=&quot;article:tag&quot; content=&quot;Technology&quot;&gt;

What Gets Extracted NowThe bookmarklet automatically captures:

Author - With fallback chain:

article:author meta tag
og:author (Open Graph)
Site name from og:site_name or application-name
Falls back to “Unknown” if not found


Title - Tries multiple sources:

Open Graph og:title
Twitter Card twitter:title
HTML &lt;title&gt; tag as last resort


Publication Date - Extracts and validates ISO format (YYYY-MM-DD):

article:published_time
datePublished meta tag
Custom publish_date meta tag


Tags - Extracts all article:tag meta tags:

Can capture multiple tags from a single page
Pre-fills the Tags field in the form
Fully editable before generating the command


Highlighted Text - Uses window.getSelection() to capture:

Text you highlight&#x2F;select BEFORE clicking the bookmark
Preserves your selection exactly as you made it



Auto-Fetch Metadata from URLBoth the bookmarklet and create-quote-post.js now also intelligently fetch missing metadata:
# If author is missing or &quot;Unknown&quot;, the script fetches it from the URLnode tools/create-quote-post.js \  --url &quot;https://example.com/article&quot; \  --quote &quot;The quote...&quot; \  --commentary &quot;My thoughts...&quot;

The script will:

Fetch the article’s HTML
Extract author, title, date, and tags from meta tags
Log what it found: ✓ Found author: Ben Thompson
Use the extracted values for post generation

The general bookmarklet works on any website with proper meta tags, pawing the path for future automations.
The updated resources are available at:

feedly-bookmarklet.js
general-bookmarklet.js
create-quote-post.js

]]></content>
      <tags>
        <tag>Hexo</tag>
        <tag>NodeJS</tag>
        <tag>Automation</tag>
        <tag>Bookmarklet</tag>
        <tag>Meta Tags</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Mikko Koskinen - AI Is Not Replacing Jobs, It’s Replacing the Work Inside Them</title>
    <url>/2026/02/13/mikko-koskinen-ai-is-not-replacing-jobs-its-replacing-the-work-inside-them/</url>
    <content><![CDATA[Mikko Koskinen in the article AI Is Not Replacing Jobs, It’s Replacing the Work Inside Them (published February 14, 2026):

A useful comparison is software development, where this change is already well advanced. Writing code used to be a largely manual activity. Then the AI helped us write part of the code. Now AI can generate significant parts of it, suggest improvements, and assist throughout the process.
Developers have not disappeared, but their role has changed. Less time is spent writing code line by line, and more time is spent defining what should be built, guiding the system, and validating the outcome.
The work has moved up a level—from execution to intent and verification.

]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Laura Kokkarinen - How to setup SharePoint Online Sites.Selected permissions — Step by step</title>
    <url>/2026/02/13/laura-kokkarinen-how-to-setup-sharepoint-online-sitesselected-permissions-step-by-step/</url>
    <content><![CDATA[Laura Kokkarinen in the article How to setup SharePoint Online Sites.Selected permissions — Step by step (published February 14, 2026):

The process of granting these permissions is two-fold:
Grant the Sites.Selected permission for your Azure app’s managed identity to the SharePoint Online API as you normally would.Add the permission your app needs on a specific site via the Microsoft Graph API to the site entity.

Good link to have on hand.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>SharePoint</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting codemanship - Why LLMs Will Always Need An Expert In The Loop</title>
    <url>/2026/02/13/codemanship-why-llms-will-always-need-an-expert-in-the-loop/</url>
    <content><![CDATA[codemanship in the article Why LLMs Will Always Need An Expert In The Loop (published February 14, 2026):

So, there are two very likely futures we can work with (or, more accurately, around). We should not expect LLMs to get significantly more reliable, and we should not expect them to make sound decisions about modular design, dependencies and high-level architecture on non-trivial systems.
The full skinny: a software developer will be required behind the wheel for any “AI” or “agentic” workflow that’s built around LLMs.

A software developer yes, but where in the process are they needed. There is lots to be done in tooling yet. Lovable is a good example, by setting the guardrails on the platform it enables less technical users to achieve great things.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Unknown - Autonomous agents - Technique 3: Provide tools for steps your agent can&#39;t easily handle (like Agent Flows)</title>
    <url>/2026/02/13/unknown-autonomous-agents-technique-3-provide-tools-for-steps-your-agent-cant-easily-handle-like-agent-flows/</url>
    <content><![CDATA[Unknown in the article Autonomous agents - Technique 3: Provide tools for steps your agent can’t easily handle (like Agent Flows) (published February 8, 2026):

They need to call out to tools which are pre-defined and wrap up the complex details of taking actions on specific systems, integrating data, or performing precise steps in a process. Every agent framework has a ‘tools’ concept, and for Microsoft agents built with Copilot Studio, this is agent flows

Keeping post for reference. Express mode seems to be new.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>Power Automate</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Matthew Devaney - Connect To Dataverse Knowledge In Copilot Studio</title>
    <url>/2026/02/13/matthew-devaney-connect-to-dataverse-knowledge-in-copilot-studio/</url>
    <content><![CDATA[Matthew Devaney in the article Connect To Dataverse Knowledge In Copilot Studio (published January 25, 2026):

You can connect an agent to… Dataverse knowledge in Copilot Studio. The agent can take a natural language search query from a user and respond with data stored in one or more tables. Simply connecting the agent to Dataverse is not enough. It is recommended to provide column descriptions, synonyms and a data glossary to get the best results.

]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>Copilot Studio</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Jonas Rapp - My advice for Still Growing Developers</title>
    <url>/2026/02/15/jonas-rapp-my-advice-for-still-growing-developers/</url>
    <content><![CDATA[Jonas Rapp in the article My advice for Still Growing Developers (published February 15, 2026):

We who work on Power Platform usually come from one background: 1) I’m a true rookie, or 2) like me, with a lot of dev experience and knowing more than we actually need today.
Today I always start to look at what is possible to solve the issue OOTB, and only if it is impossible or there are too many workarounds that make maintainability a nightmare.
Never ignore maintainability.

We had Microsoft CRM 3.0 at work for way too long. It got deployed in production 2007 and we finally replaced it with Dynamics 365 Customer Engagement Online in 2017. So we got a good 10 years from it. I didn’t really develop on the old platform but I did make a PowerShell-module that we used a bunch for automations and also the migration.
Anyway, the platform is old, but that also means it’s a really solid base to build on.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>Power Platform</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Hemant Gaur - Public preview: Power Apps MCP and enhanced agent feed for your business applications</title>
    <url>/2026/02/15/hemant-gaur-public-preview-power-apps-mcp-and-enhanced-agent-feed-for-your-business-applications/</url>
    <content><![CDATA[Hemant Gaur in the article Public preview: Power Apps MCP and enhanced agent feed for your business applications (published February 11, 2026):

Today, we’re excited to bring the… Power Apps MCP (… Model Context Protocol) Server to public preview. The Power Apps MCP Server enables agents to automate repetitive app tasks, with human review and approval through the new task-centric enhanced agent feed. The Power Apps MCP Server also provides built‑in supervision tools that allow humans to review, assist, and take control of agent actions when needed.

I like the idea of the agent feed. One place where one can do all the human-in-the-loop tasks. No need to reinvent to wheel for all new automations with adaptive cards, approvals etc.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>Power Platform</tag>
        <tag>Copilot Studio</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Nagesh Bhat - Power Pages Server Logic (preview): Enhancements</title>
    <url>/2026/02/15/nagesh-bhat-power-pages-server-logic-preview-enhancements/</url>
    <content><![CDATA[Nagesh Bhat in the article Power Pages Server Logic (preview): Enhancements (published February 11, 2026):

As Power Pages Server Logic progresses toward general availability, we’re expanding its capabilities to address real‑world scenarios while enabling faster logic development during preview.
Here is the quick look at what’s new.

I don’t know anything about this, saving it for reference. My Power Pages experience is now 2 weeks vs the 10+ years of XRM experience.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>Power Pages</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Diana Birkelbach #PCFLady - Navigating to and from Generative Pages</title>
    <url>/2026/02/15/diana-birkelbach-pcflady-navigating-to-and-from-generative-pages/</url>
    <content><![CDATA[Diana Birkelbach #PCFLady in the article Navigating to and from Generative Pages (published February 15, 2026):

We can use the navigateTo API as long we don’t need to pass parameters to the GenPage, or use the URL if we need them. When using the data URL parameters, we need to take care that those parameters are not necessary passed to the current GenPage; it depends how the page was opened. It could be the URL from the from or other hosting opener.

Still waiting for these to reach EU tenants. In the meantime vibe coding PCF-controls is an option.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>Power Platform</tag>
        <tag>Model Driven Apps</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Birgitta Böckeler - Harness Engineering</title>
    <url>/2026/02/17/birgitta-bckeler-harness-engineering/</url>
    <content><![CDATA[Birgitta Böckeler in the article Harness Engineering (published February 18, 2026):

As coding becomes less about typing code and more about steering its generation, AI might push us toward fewer tech stacks. Usability of frameworks and SDKs still matters — we’re seeing repeatedly that what’s good for humans is good for AI. But developer tastes will matter less at that level of detail. Little inefficiencies and idiosyncracies in interfaces will be less annoying since we don’t deal with them directly. We might choose stacks with good harnesses available and prioritize “AI-friendliness”.

I do wonder about this. There is also an incentive for platform developers to make their platforms more AI friendly.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI Coding</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Microsoft Defender Security Research Team - Running OpenClaw safely: identity, isolation, and runtime risk</title>
    <url>/2026/02/18/microsoft-defender-security-research-team-running-openclaw-safely-identity-isolation-and-runtime-risk/</url>
    <content><![CDATA[Microsoft Defender Security Research Team in the article Running OpenClaw safely: identity, isolation, and runtime risk (published February 19, 2026):

Self-hosted agent runtimes like… OpenClaw are showing up fast in enterprise pilots, and they introduce a blunt reality: OpenClaw includes limited built-in security controls. The runtime can ingest untrusted text, download and execute skills (i.e. code) from external sources, and perform actions using the credentials assigned to it.

Making secure environments for agents is a must.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Quoting Martin Fowler - Fragments: February 19</title>
    <url>/2026/02/18/martin-fowler-fragments-february-19/</url>
    <content><![CDATA[Martin Fowler in the article Fragments: February 19 (published February 19, 2026):

The point here is that LLM’s vulnerability is currently unfixable, they are gullible and easily manipulated into Initial Access. As one friend put it “this is the first technology we’ve built that’s subject to social engineering”. The kill chain gives us a framework to build a defensive strategy.

There might not be a fix and that will limit the use of AI in many use cases once this becomes a wide spread problem. As more people employ AI agents attackers will follow.
]]></content>
      <tags>
        <tag>Quote</tag>
        <tag>AI</tag>
      </tags>
  </entry>
</search>
